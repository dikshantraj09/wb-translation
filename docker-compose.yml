volumes:
  n8n_oss_storage:
  postgres_oss_storage:
  ollama_data:
  whisper_cache:

networks:
  oss_demo:

services:
  postgres-oss:
    image: postgres:16-alpine
    networks: ['oss_demo']
    restart: unless-stopped
    environment:
      POSTGRES_USER: n8n
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: n8n
    volumes:
      - postgres_oss_storage:/var/lib/postgresql/data

  n8n-oss:
    image: n8nio/n8n:latest
    networks: ['oss_demo']
    restart: unless-stopped
    ports:
      - 5678:5678
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: postgres-oss
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_DATABASE: n8n
      DB_POSTGRESDB_USER: n8n
      DB_POSTGRESDB_PASSWORD: n8n
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}
    volumes:
      - n8n_oss_storage:/home/node/.n8n

  whisper-asr:
    image: onerahmet/openai-whisper-asr-webservice:latest #-gpu
    container_name: whisper-asr
    networks: ['oss_demo']
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}  # Options: tiny, base, small, medium, large
      - ASR_ENGINE=${WHISPER_ENGINE:-openai_whisper}  # Options: openai_whisper, faster_whisper
      - ASR_MODEL_PATH=/data/whisper
    volumes:
      - whisper_cache:/data/whisper  # Persist model cache to avoid redownloading
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    # profiles: ["gpu-nvidia"]  # Only run with GPU profile

  ollama-pull:
    image: ollama/ollama:latest
    container_name: ollama-pull
    networks: ['oss_demo']
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: /bin/sh
    command:
      - "-c"
      - "sleep 3; OLLAMA_HOST=ollama:11434 ollama pull qwen3:4b;"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks: ['oss_demo']
    restart: unless-stopped
    expose:
      - 11434/tcp
    volumes:
      - ollama_data:/root/.ollama
    depends_on:
      - ollama-pull
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]